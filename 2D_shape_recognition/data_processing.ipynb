{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Data is loaded from this paper: https://www.sciencedirect.com/science/article/pii/S2352340920309847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found: 90000\n",
      "Example files\n",
      "data/shapes/Nonagon_fb1f8b2e-2a8d-11ea-8123-8363a7ec19e6.png\n",
      "data/shapes/Hexagon_95d6bf6e-2a95-11ea-8123-8363a7ec19e6.png\n",
      "data/shapes/Square_037dc95e-2a91-11ea-8123-8363a7ec19e6.png\n",
      "data/shapes/Triangle_2f39dc38-2a94-11ea-8123-8363a7ec19e6.png\n",
      "data/shapes/Square_398778f2-2a9a-11ea-8123-8363a7ec19e6.png\n"
     ]
    }
   ],
   "source": [
    "# Number of files in the data/shapes\n",
    "image_files = list(Path('data/shapes').glob('*.png'))\n",
    "print('Number of images found:', len(image_files))\n",
    "\n",
    "# Print names of first five files\n",
    "print(\"Example files\")\n",
    "print('\\n'.join([str(x) for x in image_files[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the shape label is included the filename. We can also check this by visualizing the first shape in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test image\n",
    "\n",
    "Below is an example of what an image in the dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: Hexagon\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAIAAAAiOjnJAAAFZUlEQVR4Ae3cwXHbVhRA0SiTStKEdlFVqSJ9qJAUklriTGY09pAGSeDdL5I6XskE8PBx/wG108vb++sv/ikwXeDX6YHmKfBfAbA4SAqAlWQ1FCwGkgJgJVkNBYuBpABYSVZDwWIgKQBWktVQsBhICoCVZDUULAaSAmAlWQ0Fi4GkAFhJVkPBYiApAFaS1VCwGEgKgJVkNRQsBpICYCVZDQWLgaQAWElWQ8FiICkAVpLVULAYSAqAlWQ1FCwGkgJgJVkNBYuBpABYSVZDwWIgKQBWktVQsBhICoCVZDUULAaSAmAlWQ0Fi4GkAFhJVkPBYiApAFaS1VCwGEgKgJVkNRQsBpICYCVZDQWLgaQAWElWQ8FiICkAVpLVULAYSAqAlWQ1FCwGkgJgJVkNBYuBpABYSVZDwWIgKQBWktVQsBhICoCVZDUULAaSAmAlWQ0Fi4GkAFhJVkPBYiApAFaS1VCwGEgKgJVkNRQsBpICYCVZDQWLgaQAWElWQ8FiICkAVpLVULAYSAqAlWQ1FCwGkgJgJVkNBYuBpABYSVZDwWIgKQBWktVQsBhICoCVZDUULAaSAmAlWQ0Fi4GkAFhJVkPBYiApAFaS1VCwGEgKgJVkNRQsBpICYCVZDQWLgaQAWElWQ8FiICkAVpLVULAYSAqAlWQ1FCwGkgJgJVkNBYuBpABYSVZDwWIgKQBWktVQsBhICoCVZDUULAaSAmAlWQ0Fi4GkAFhJVkPBYiApAFaS1VCwGEgKgJVkNRQsBpICYCVZDQWLgaQAWElWQ8FiICkAVpLV0N8keNwCf/z+5zWL//ufv645bfacl7f319mJpl0scCWIi3OuP2G9Lb8Kr9+dmTPXq5pZ941TwLox2GOevl4zWEulrN/gj8dbfGuwPsr7YbIAWJM1t2ct/s44XczKBYB12v+ZP1lmC6xFjJbt6KLnuXQbsC4Verrja4iDtQLOmr1c8SRX3wOsq1M90YkLoIOVe1mwizueoV4VWDs2xSWXC4B1udGRM+ovhrtdG1hHtubhr+3cgxXi6LYtXPTQaLCGQp6MeRRV0TrBOhHhg4kCYE1UPJkRfQ2c3Gfmg2K1YM3szaNPGbcF1jyJ8U2aX2I/Eay+8YPcYfZ9AGt422e3Z3hxl8YNLh6sS7Ed31UArF3ZfnLR4Bv/kzvkH089Alj5Vn3NG4A1tu9T7/rYgvYOGnkQsPbmf+rrjtsCawbI8Z2YWcfdTAHrbrbizhZy8FUBa2A/D+7BwArubwRYR/fkWVUd/MtHYB2F5fqzBcA6m+WGDw++2TfcaeGpxx8KrIXb9ZVuBdbAbh9/vwcWMTdi5HHAmtsQk74rANZ3MQ78OPKWH7j/2KVTDwLW3W3J2II+dRBYn5r/zm4+9XX17bHAmtzbwY2ZXNZnzAJruPrj2ppdOVjDsIz7vwBY8xJmX/359Z2bOL5msM5l/mKfjav61g+sBFGxVclCs6FgVWkfxVa0TrAqWF98LlghgOjLYHDF3QrBGtymM6O6nTtzs3v6CKx72o21a0nRg5VvZrp/+er33gCsveUe/LqaO1grgNS7uOIZbrwHWDcG23v6XdlasBiw9kpx3WYBsDbzjB5c8D1xzXrXLAOsa/Zi7Jw1mzq23AODwDoQ7wEvXSYbrNU6lm3t6gf78X5g/djjqf+30jRYn0Bp5QZ/PN7im768vb9+3NsPKwsUf/9osZ6NXGBtxHFofwG/Cve3c+VGAbA24ji0vwBY+9u5cqMAWBtxHNpfAKz97Vy5UQCsjTgO7S8A1v52rtwoANZGHIf2FwBrfztXbhQAayOOQ/sLgLW/nSs3CoC1Eceh/QXA2t/OlRsF/gWdQXf/tzSP9wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=200x200>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the first image from data/shapes\n",
    "first_img = image_files[1]\n",
    "\n",
    "# Get the label from the filename\n",
    "label = first_img.parts[-1].split(\"_\")[0]\n",
    "\n",
    "print(f\"Shape: {label}\")\n",
    "image = Image.open(first_img)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image seems to be a color image (with 3 channels). This can be confirmed by checking the shape of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images seem to be in a simple 200 x 200 shape. We dont need the colorscale for classifying simple images, so we could simply convert the images to grayscale. The interesting thing in terms of modeling is that the shapes come in different background colors, meaning that the \"non-shape\" area wont always be white by default. However, this shouldnt eventually prove to be any problem since the model should anyways catch the shape differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[123, 123, 123, ..., 123, 123, 123],\n",
       "       [123, 123, 123, ..., 123, 123, 123],\n",
       "       [123, 123, 123, ..., 123, 123, 123],\n",
       "       ...,\n",
       "       [123, 123, 123, ..., 123, 123, 123],\n",
       "       [123, 123, 123, ..., 123, 123, 123],\n",
       "       [123, 123, 123, ..., 123, 123, 123]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # Convert the image to grayscale\n",
    "image = image.convert('L')\n",
    "image\n",
    "\n",
    "np.array(image)\n",
    "\n",
    "# # Resize the image to a specific size\n",
    "# image = image.resize((224, 224))\n",
    "\n",
    "# # Convert the image to a numpy array\n",
    "# image_array = np.array(image)\n",
    "\n",
    "# # Normalize the pixel values\n",
    "# image_array = image_array / 255.0\n",
    "\n",
    "# # Reshape the image array to match the input shape of the neural network\n",
    "# image_array = np.reshape(image_array, (1, 224, 224, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Next we automate the similar processing for all of the images and save them for later training. We will also split the data into train and validation sets. We will also normalize the pixel values to be 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482903696474494286a20fc38a4bb43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data_folder = \"data/\"\n",
    "label_list = []\n",
    "img_list = []\n",
    "\n",
    "for file_name in tqdm(image_files[:1000]):\n",
    "    # Get shape label\n",
    "    label = str(file_name).split(\"_\")[0]\n",
    "    label_list.append(label)\n",
    "\n",
    "    # Get image\n",
    "    image = Image.open(first_img).convert('L')\n",
    "    image = np.array(image) / 255.0\n",
    "    img_list.append(image)\n",
    "\n",
    "# print(label_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets split the data and save them for later training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 800\n",
      "Test set size: 200\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(img_list, label_list, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of the train and test sets\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder data/processed with pathlib\n",
    "processed_folder = Path(\"data\") / \"processed\"\n",
    "processed_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save results to processed folder\n",
    "np.save(processed_folder / 'X_train.npy', X_train)\n",
    "np.save(processed_folder / 'X_test.npy', X_test)\n",
    "np.save(processed_folder / 'y_train.npy', y_train)\n",
    "np.save(processed_folder / 'y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200, 200)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.load(\"X_train.npy\")\n",
    "X_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shape_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
